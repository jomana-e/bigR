% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/big_read_csv.R
\name{big_read_csv}
\alias{big_read_csv}
\title{Read a large CSV file using an optimized backend}
\usage{
big_read_csv(file, backend = "auto", chunk_size = NULL)
}
\arguments{
\item{file}{Path to the CSV file}

\item{backend}{Choose backend: "auto", "data.table", "arrow", or "duckdb"}

\item{chunk_size}{Optional chunk size for out-of-core processing}
}
\value{
A dataframe or a DuckDB connection
}
\description{
Read a large CSV file using an optimized backend
}
\examples{
\dontrun{
# Basic usage with automatic backend selection
data <- big_read_csv("large_dataset.csv")

# Specify a backend explicitly
data <- big_read_csv("large_dataset.csv", backend = "data.table")

# Read a very large file with DuckDB
data <- big_read_csv("huge_dataset.csv", backend = "duckdb")

# Process data in chunks
data <- big_read_csv("large_dataset.csv", chunk_size = 10000)
}
}
