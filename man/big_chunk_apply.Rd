% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/big_chunk_apply.R
\name{big_chunk_apply}
\alias{big_chunk_apply}
\title{Apply a function to a large dataset in chunks}
\usage{
big_chunk_apply(file, FUN, chunk_size = 10000)
}
\arguments{
\item{file}{Path to the CSV file}

\item{FUN}{Function to apply to each chunk}

\item{chunk_size}{Number of rows per chunk}
}
\value{
Combined result after applying the function
}
\description{
Apply a function to a large dataset in chunks
}
\examples{
\dontrun{
# Simple transformation on chunks
result <- big_chunk_apply("large_data.csv",
  FUN = function(chunk) {
    chunk \%>\%
      dplyr::mutate(new_col = value * 2)
  }
)

# Process chunks with custom chunk size
result <- big_chunk_apply("large_data.csv",
  FUN = function(chunk) {
    chunk \%>\%
      dplyr::filter(value > 0) \%>\%
      dplyr::mutate(log_value = log(value))
  },
  chunk_size = 5000
)

# Complex chunk processing with aggregation
result <- big_chunk_apply("large_data.csv",
  FUN = function(chunk) {
    chunk \%>\%
      dplyr::group_by(category) \%>\%
      dplyr::summarize(
        mean_val = mean(value),
        count = n()
      )
  }
)
}
}
